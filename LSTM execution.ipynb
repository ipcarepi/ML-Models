{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV files:\n",
      "  Reading: differential_cy_easier.csv\n",
      "  Reading: differential_cy_easy.csv\n",
      "  Reading: differential_cy_hard.csv\n",
      "  Reading: differential_cy_harder.csv\n",
      "  Reading: differential_cy_moderate.csv\n",
      "  Reading: differential_dh_easier.csv\n",
      "  Reading: differential_dh_easy.csv\n",
      "  Reading: differential_dh_hard.csv\n",
      "  Reading: differential_dh_harder.csv\n",
      "  Reading: differential_dh_moderate.csv\n",
      "  Reading: differential_gb_easier.csv\n",
      "  Reading: differential_gb_easy.csv\n",
      "  Reading: differential_gb_hard.csv\n",
      "  Reading: differential_gb_harder.csv\n",
      "  Reading: differential_gb_moderate.csv\n",
      "  Reading: differential_jj_easier.csv\n",
      "  Reading: differential_jj_easy.csv\n",
      "  Reading: differential_jj_hard.csv\n",
      "  Reading: differential_jj_harder.csv\n",
      "  Reading: differential_jj_moderate.csv\n",
      "  Reading: differential_jw_easier.csv\n",
      "  Reading: differential_jw_easy.csv\n",
      "  Reading: differential_jw_hard.csv\n",
      "  Reading: differential_jw_harder.csv\n",
      "  Reading: differential_jw_moderate.csv\n",
      "  Reading: differential_kb_easier.csv\n",
      "  Reading: differential_kb_easy.csv\n",
      "  Reading: differential_kb_hard.csv\n",
      "  Reading: differential_kb_harder.csv\n",
      "  Reading: differential_kb_moderate.csv\n",
      "  Reading: differential_kw_easier.csv\n",
      "  Reading: differential_kw_easy.csv\n",
      "  Reading: differential_kw_hard.csv\n",
      "  Reading: differential_kw_harder.csv\n",
      "  Reading: differential_kw_moderate.csv\n",
      "  Reading: differential_mw_easier.csv\n",
      "  Reading: differential_mw_easy.csv\n",
      "  Reading: differential_mw_hard.csv\n",
      "  Reading: differential_mw_harder.csv\n",
      "  Reading: differential_mw_moderate.csv\n",
      "  Reading: differential_sh_easier.csv\n",
      "  Reading: differential_sh_easy.csv\n",
      "  Reading: differential_sh_hard.csv\n",
      "  Reading: differential_sh_harder.csv\n",
      "  Reading: differential_sh_moderate.csv\n",
      "  Reading: differential_sm_easier.csv\n",
      "  Reading: differential_sm_easy.csv\n",
      "  Reading: differential_sm_hard.csv\n",
      "  Reading: differential_sm_harder.csv\n",
      "  Reading: differential_sm_moderate.csv\n",
      "  Reading: differential_ym_easier.csv\n",
      "  Reading: differential_ym_easy.csv\n",
      "  Reading: differential_ym_hard.csv\n",
      "  Reading: differential_ym_harder.csv\n",
      "  Reading: differential_ym_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_cy_easier.csv\n",
      "  Reading: E4 BVP, IBI data_cy_easy.csv\n",
      "  Reading: E4 BVP, IBI data_cy_hard.csv\n",
      "  Reading: E4 BVP, IBI data_cy_harder.csv\n",
      "  Reading: E4 BVP, IBI data_cy_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_dh_easier.csv\n",
      "  Reading: E4 BVP, IBI data_dh_easy.csv\n",
      "  Reading: E4 BVP, IBI data_dh_hard.csv\n",
      "  Reading: E4 BVP, IBI data_dh_harder.csv\n",
      "  Reading: E4 BVP, IBI data_dh_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_gb_easier.csv\n",
      "  Reading: E4 BVP, IBI data_gb_easy.csv\n",
      "  Reading: E4 BVP, IBI data_gb_hard.csv\n",
      "  Reading: E4 BVP, IBI data_gb_harder.csv\n",
      "  Reading: E4 BVP, IBI data_gb_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_jj_easier.csv\n",
      "  Reading: E4 BVP, IBI data_jj_easy.csv\n",
      "  Reading: E4 BVP, IBI data_jj_hard.csv\n",
      "  Reading: E4 BVP, IBI data_jj_harder.csv\n",
      "  Reading: E4 BVP, IBI data_jj_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_jw_easier.csv\n",
      "  Reading: E4 BVP, IBI data_jw_easy.csv\n",
      "  Reading: E4 BVP, IBI data_jw_hard.csv\n",
      "  Reading: E4 BVP, IBI data_jw_harder.csv\n",
      "  Reading: E4 BVP, IBI data_jw_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_kb_easier.csv\n",
      "  Reading: E4 BVP, IBI data_kb_easy.csv\n",
      "  Reading: E4 BVP, IBI data_kb_hard.csv\n",
      "  Reading: E4 BVP, IBI data_kb_harder.csv\n",
      "  Reading: E4 BVP, IBI data_kb_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_kw_easier.csv\n",
      "  Reading: E4 BVP, IBI data_kw_easy.csv\n",
      "  Reading: E4 BVP, IBI data_kw_hard.csv\n",
      "  Reading: E4 BVP, IBI data_kw_harder.csv\n",
      "  Reading: E4 BVP, IBI data_kw_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_mw_easier.csv\n",
      "  Reading: E4 BVP, IBI data_mw_easy.csv\n",
      "  Reading: E4 BVP, IBI data_mw_hard.csv\n",
      "  Reading: E4 BVP, IBI data_mw_harder.csv\n",
      "  Reading: E4 BVP, IBI data_mw_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_sh_easier.csv\n",
      "  Reading: E4 BVP, IBI data_sh_easy.csv\n",
      "  Reading: E4 BVP, IBI data_sh_hard.csv\n",
      "  Reading: E4 BVP, IBI data_sh_harder.csv\n",
      "  Reading: E4 BVP, IBI data_sh_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_sm_easier.csv\n",
      "  Reading: E4 BVP, IBI data_sm_easy.csv\n",
      "  Reading: E4 BVP, IBI data_sm_hard.csv\n",
      "  Reading: E4 BVP, IBI data_sm_harder.csv\n",
      "  Reading: E4 BVP, IBI data_sm_moderate.csv\n",
      "  Reading: E4 BVP, IBI data_ym_easier.csv\n",
      "  Reading: E4 BVP, IBI data_ym_easy.csv\n",
      "  Reading: E4 BVP, IBI data_ym_hard.csv\n",
      "  Reading: E4 BVP, IBI data_ym_harder.csv\n",
      "  Reading: E4 BVP, IBI data_ym_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_cy_easier.csv\n",
      "  Reading: E4 GSR, TMP data_cy_easy.csv\n",
      "  Reading: E4 GSR, TMP data_cy_hard.csv\n",
      "  Reading: E4 GSR, TMP data_cy_harder.csv\n",
      "  Reading: E4 GSR, TMP data_cy_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_dh_easier.csv\n",
      "  Reading: E4 GSR, TMP data_dh_easy.csv\n",
      "  Reading: E4 GSR, TMP data_dh_hard.csv\n",
      "  Reading: E4 GSR, TMP data_dh_harder.csv\n",
      "  Reading: E4 GSR, TMP data_dh_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_gb_easier.csv\n",
      "  Reading: E4 GSR, TMP data_gb_easy.csv\n",
      "  Reading: E4 GSR, TMP data_gb_hard.csv\n",
      "  Reading: E4 GSR, TMP data_gb_harder.csv\n",
      "  Reading: E4 GSR, TMP data_gb_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_jj_easier.csv\n",
      "  Reading: E4 GSR, TMP data_jj_easy.csv\n",
      "  Reading: E4 GSR, TMP data_jj_hard.csv\n",
      "  Reading: E4 GSR, TMP data_jj_harder.csv\n",
      "  Reading: E4 GSR, TMP data_jj_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_jw_easier.csv\n",
      "  Reading: E4 GSR, TMP data_jw_easy.csv\n",
      "  Reading: E4 GSR, TMP data_jw_hard.csv\n",
      "  Reading: E4 GSR, TMP data_jw_harder.csv\n",
      "  Reading: E4 GSR, TMP data_jw_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_kb_easier.csv\n",
      "  Reading: E4 GSR, TMP data_kb_easy.csv\n",
      "  Reading: E4 GSR, TMP data_kb_hard.csv\n",
      "  Reading: E4 GSR, TMP data_kb_harder.csv\n",
      "  Reading: E4 GSR, TMP data_kb_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_kw_easier.csv\n",
      "  Reading: E4 GSR, TMP data_kw_easy.csv\n",
      "  Reading: E4 GSR, TMP data_kw_hard.csv\n",
      "  Reading: E4 GSR, TMP data_kw_harder.csv\n",
      "  Reading: E4 GSR, TMP data_kw_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_mw_easier.csv\n",
      "  Reading: E4 GSR, TMP data_mw_easy.csv\n",
      "  Reading: E4 GSR, TMP data_mw_hard.csv\n",
      "  Reading: E4 GSR, TMP data_mw_harder.csv\n",
      "  Reading: E4 GSR, TMP data_mw_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_sh_easier.csv\n",
      "  Reading: E4 GSR, TMP data_sh_easy.csv\n",
      "  Reading: E4 GSR, TMP data_sh_hard.csv\n",
      "  Reading: E4 GSR, TMP data_sh_harder.csv\n",
      "  Reading: E4 GSR, TMP data_sh_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_sm_easier.csv\n",
      "  Reading: E4 GSR, TMP data_sm_easy.csv\n",
      "  Reading: E4 GSR, TMP data_sm_hard.csv\n",
      "  Reading: E4 GSR, TMP data_sm_harder.csv\n",
      "  Reading: E4 GSR, TMP data_sm_moderate.csv\n",
      "  Reading: E4 GSR, TMP data_ym_easier.csv\n",
      "  Reading: E4 GSR, TMP data_ym_easy.csv\n",
      "  Reading: E4 GSR, TMP data_ym_hard.csv\n",
      "  Reading: E4 GSR, TMP data_ym_harder.csv\n",
      "  Reading: E4 GSR, TMP data_ym_moderate.csv\n",
      "  Reading: Eye Feature data_cy_easier.csv\n",
      "  Reading: Eye Feature data_cy_easy.csv\n",
      "  Reading: Eye Feature data_cy_hard.csv\n",
      "  Reading: Eye Feature data_cy_harder.csv\n",
      "  Reading: Eye Feature data_cy_moderate.csv\n",
      "  Reading: Eye Feature data_dh_easier.csv\n",
      "  Reading: Eye Feature data_dh_easy.csv\n",
      "  Reading: Eye Feature data_dh_hard.csv\n",
      "  Reading: Eye Feature data_dh_harder.csv\n",
      "  Reading: Eye Feature data_dh_moderate.csv\n",
      "  Reading: Eye Feature data_gb_easier.csv\n",
      "  Reading: Eye Feature data_gb_easy.csv\n",
      "  Reading: Eye Feature data_gb_hard.csv\n",
      "  Reading: Eye Feature data_gb_harder.csv\n",
      "  Reading: Eye Feature data_gb_moderate.csv\n",
      "  Reading: Eye Feature data_jj_easier.csv\n",
      "  Reading: Eye Feature data_jj_easy.csv\n",
      "  Reading: Eye Feature data_jj_hard.csv\n",
      "  Reading: Eye Feature data_jj_harder.csv\n",
      "  Reading: Eye Feature data_jj_moderate.csv\n",
      "  Reading: Eye Feature data_jw_easier.csv\n",
      "  Reading: Eye Feature data_jw_easy.csv\n",
      "  Reading: Eye Feature data_jw_hard.csv\n",
      "  Reading: Eye Feature data_jw_harder.csv\n",
      "  Reading: Eye Feature data_jw_moderate.csv\n",
      "  Reading: Eye Feature data_kb_easier.csv\n",
      "  Reading: Eye Feature data_kb_easy.csv\n",
      "  Reading: Eye Feature data_kb_hard.csv\n",
      "  Reading: Eye Feature data_kb_harder.csv\n",
      "  Reading: Eye Feature data_kb_moderate.csv\n",
      "  Reading: Eye Feature data_kw_easier.csv\n",
      "  Reading: Eye Feature data_kw_easy.csv\n",
      "  Reading: Eye Feature data_kw_hard.csv\n",
      "  Reading: Eye Feature data_kw_harder.csv\n",
      "  Reading: Eye Feature data_kw_moderate.csv\n",
      "  Reading: Eye Feature data_mw_easier.csv\n",
      "  Reading: Eye Feature data_mw_easy.csv\n",
      "  Reading: Eye Feature data_mw_hard.csv\n",
      "  Reading: Eye Feature data_mw_harder.csv\n",
      "  Reading: Eye Feature data_mw_moderate.csv\n",
      "  Reading: Eye Feature data_sh_easier.csv\n",
      "  Reading: Eye Feature data_sh_easy.csv\n",
      "  Reading: Eye Feature data_sh_hard.csv\n",
      "  Reading: Eye Feature data_sh_harder.csv\n",
      "  Reading: Eye Feature data_sh_moderate.csv\n",
      "  Reading: Eye Feature data_sm_easier.csv\n",
      "  Reading: Eye Feature data_sm_easy.csv\n",
      "  Reading: Eye Feature data_sm_hard.csv\n",
      "  Reading: Eye Feature data_sm_harder.csv\n",
      "  Reading: Eye Feature data_sm_moderate.csv\n",
      "  Reading: Eye Feature data_ym_easier.csv\n",
      "  Reading: Eye Feature data_ym_easy.csv\n",
      "  Reading: Eye Feature data_ym_hard.csv\n",
      "  Reading: Eye Feature data_ym_harder.csv\n",
      "  Reading: Eye Feature data_ym_moderate.csv\n",
      "  Reading: Eye Sensor data_cy_easier.csv\n",
      "  Reading: Eye Sensor data_cy_easy.csv\n",
      "  Reading: Eye Sensor data_cy_hard.csv\n",
      "  Reading: Eye Sensor data_cy_harder.csv\n",
      "  Reading: Eye Sensor data_cy_moderate.csv\n",
      "  Reading: Eye Sensor data_dh_easier.csv\n",
      "  Reading: Eye Sensor data_dh_easy.csv\n",
      "  Reading: Eye Sensor data_dh_hard.csv\n",
      "  Reading: Eye Sensor data_dh_harder.csv\n",
      "  Reading: Eye Sensor data_dh_moderate.csv\n",
      "  Reading: Eye Sensor data_gb_easier.csv\n",
      "  Reading: Eye Sensor data_gb_easy.csv\n",
      "  Reading: Eye Sensor data_gb_hard.csv\n",
      "  Reading: Eye Sensor data_gb_harder.csv\n",
      "  Reading: Eye Sensor data_gb_moderate.csv\n",
      "  Reading: Eye Sensor data_jj_easier.csv\n",
      "  Reading: Eye Sensor data_jj_easy.csv\n",
      "  Reading: Eye Sensor data_jj_hard.csv\n",
      "  Reading: Eye Sensor data_jj_harder.csv\n",
      "  Reading: Eye Sensor data_jj_moderate.csv\n",
      "  Reading: Eye Sensor data_jw_easier.csv\n",
      "  Reading: Eye Sensor data_jw_easy.csv\n",
      "  Reading: Eye Sensor data_jw_hard.csv\n",
      "  Reading: Eye Sensor data_jw_harder.csv\n",
      "  Reading: Eye Sensor data_jw_moderate.csv\n",
      "  Reading: Eye Sensor data_kb_easier.csv\n",
      "  Reading: Eye Sensor data_kb_easy.csv\n",
      "  Reading: Eye Sensor data_kb_hard.csv\n",
      "  Reading: Eye Sensor data_kb_harder.csv\n",
      "  Reading: Eye Sensor data_kb_moderate.csv\n",
      "  Reading: Eye Sensor data_kw_easier.csv\n",
      "  Reading: Eye Sensor data_kw_easy.csv\n",
      "  Reading: Eye Sensor data_kw_hard.csv\n",
      "  Reading: Eye Sensor data_kw_harder.csv\n",
      "  Reading: Eye Sensor data_kw_moderate.csv\n",
      "  Reading: Eye Sensor data_mw_easier.csv\n",
      "  Reading: Eye Sensor data_mw_easy.csv\n",
      "  Reading: Eye Sensor data_mw_hard.csv\n",
      "  Reading: Eye Sensor data_mw_harder.csv\n",
      "  Reading: Eye Sensor data_mw_moderate.csv\n",
      "  Reading: Eye Sensor data_sh_easier.csv\n",
      "  Reading: Eye Sensor data_sh_easy.csv\n",
      "  Reading: Eye Sensor data_sh_hard.csv\n",
      "  Reading: Eye Sensor data_sh_harder.csv\n",
      "  Reading: Eye Sensor data_sh_moderate.csv\n",
      "  Reading: Eye Sensor data_sm_easier.csv\n",
      "  Reading: Eye Sensor data_sm_easy.csv\n",
      "  Reading: Eye Sensor data_sm_hard.csv\n",
      "  Reading: Eye Sensor data_sm_harder.csv\n",
      "  Reading: Eye Sensor data_sm_moderate.csv\n",
      "  Reading: Eye Sensor data_ym_easier.csv\n",
      "  Reading: Eye Sensor data_ym_easy.csv\n",
      "  Reading: Eye Sensor data_ym_hard.csv\n",
      "  Reading: Eye Sensor data_ym_harder.csv\n",
      "  Reading: Eye Sensor data_ym_moderate.csv\n",
      "  Reading: game_data_jw_easier.csv\n",
      "  Reading: game_data_jw_easy.csv\n",
      "  Reading: game_data_jw_hard.csv\n",
      "  Reading: game_data_jw_moderate.csv\n",
      "  Reading: game_data_mw_easier.csv\n",
      "  Reading: game_data_mw_hard.csv\n",
      "  Reading: game_data_mw_harder.csv\n",
      "  Reading: game_data_sm_easier.csv\n",
      "  Reading: game_data_sm_easy.csv\n",
      "  Reading: game_data_sm_hard.csv\n",
      "  Reading: game_data_sm_harder.csv\n",
      "  Reading: game_data_sm_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_cy_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_cy_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_cy_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_cy_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_cy_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_dh_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_dh_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_dh_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_dh_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_dh_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_gb_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_gb_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_gb_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_gb_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_gb_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_jj_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_jj_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_jj_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_jj_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_jj_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_jw_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_jw_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_jw_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_jw_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_jw_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_kb_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_kb_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_kb_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_kb_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_kb_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_kw_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_kw_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_kw_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_kw_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_kw_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_mw_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_mw_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_mw_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_mw_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_mw_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_sh_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_sh_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_sh_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_sh_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_sh_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_sm_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_sm_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_sm_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_sm_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_sm_moderate.csv\n",
      "  Reading: Peak Saccadic Velocity data_ym_easier.csv\n",
      "  Reading: Peak Saccadic Velocity data_ym_easy.csv\n",
      "  Reading: Peak Saccadic Velocity data_ym_hard.csv\n",
      "  Reading: Peak Saccadic Velocity data_ym_harder.csv\n",
      "  Reading: Peak Saccadic Velocity data_ym_moderate.csv\n",
      "  Reading: performance_cy_easier.csv\n",
      "  Reading: performance_cy_easy.csv\n",
      "  Reading: performance_cy_hard.csv\n",
      "  Reading: performance_cy_harder.csv\n",
      "  Reading: performance_cy_moderate.csv\n",
      "  Reading: performance_dh_easier.csv\n",
      "  Reading: performance_dh_easy.csv\n",
      "  Reading: performance_dh_hard.csv\n",
      "  Reading: performance_dh_harder.csv\n",
      "  Reading: performance_dh_moderate.csv\n",
      "  Reading: performance_gb_easier.csv\n",
      "  Reading: performance_gb_easy.csv\n",
      "  Reading: performance_gb_hard.csv\n",
      "  Reading: performance_gb_harder.csv\n",
      "  Reading: performance_gb_moderate.csv\n",
      "  Reading: performance_jj_easier.csv\n",
      "  Reading: performance_jj_easy.csv\n",
      "  Reading: performance_jj_hard.csv\n",
      "  Reading: performance_jj_harder.csv\n",
      "  Reading: performance_jj_moderate.csv\n",
      "  Reading: performance_jw_harder.csv\n",
      "  Reading: performance_kb_easier.csv\n",
      "  Reading: performance_kb_easy.csv\n",
      "  Reading: performance_kb_hard.csv\n",
      "  Reading: performance_kb_harder.csv\n",
      "  Reading: performance_kb_moderate.csv\n",
      "  Reading: performance_kw_easier.csv\n",
      "  Reading: performance_kw_easy.csv\n",
      "  Reading: performance_kw_hard.csv\n",
      "  Reading: performance_kw_harder.csv\n",
      "  Reading: performance_kw_moderate.csv\n",
      "  Reading: performance_mw_easy.csv\n",
      "  Reading: performance_mw_moderate.csv\n",
      "  Reading: performance_sh_easier.csv\n",
      "  Reading: performance_sh_easy.csv\n",
      "  Reading: performance_sh_hard.csv\n",
      "  Reading: performance_sh_harder.csv\n",
      "  Reading: performance_sh_moderate.csv\n",
      "  Reading: performance_ym_easier.csv\n",
      "  Reading: performance_ym_easy.csv\n",
      "  Reading: performance_ym_hard.csv\n",
      "  Reading: performance_ym_harder.csv\n",
      "  Reading: performance_ym_moderate.csv\n",
      "  Reading: Saccade Amplitude data_cy_easier.csv\n",
      "  Reading: Saccade Amplitude data_cy_easy.csv\n",
      "  Reading: Saccade Amplitude data_cy_hard.csv\n",
      "  Reading: Saccade Amplitude data_cy_harder.csv\n",
      "  Reading: Saccade Amplitude data_cy_moderate.csv\n",
      "  Reading: Saccade Amplitude data_dh_easier.csv\n",
      "  Reading: Saccade Amplitude data_dh_easy.csv\n",
      "  Reading: Saccade Amplitude data_dh_hard.csv\n",
      "  Reading: Saccade Amplitude data_dh_harder.csv\n",
      "  Reading: Saccade Amplitude data_dh_moderate.csv\n",
      "  Reading: Saccade Amplitude data_gb_easier.csv\n",
      "  Reading: Saccade Amplitude data_gb_easy.csv\n",
      "  Reading: Saccade Amplitude data_gb_hard.csv\n",
      "  Reading: Saccade Amplitude data_gb_harder.csv\n",
      "  Reading: Saccade Amplitude data_gb_moderate.csv\n",
      "  Reading: Saccade Amplitude data_jj_easier.csv\n",
      "  Reading: Saccade Amplitude data_jj_easy.csv\n",
      "  Reading: Saccade Amplitude data_jj_hard.csv\n",
      "  Reading: Saccade Amplitude data_jj_harder.csv\n",
      "  Reading: Saccade Amplitude data_jj_moderate.csv\n",
      "  Reading: Saccade Amplitude data_jw_easier.csv\n",
      "  Reading: Saccade Amplitude data_jw_easy.csv\n",
      "  Reading: Saccade Amplitude data_jw_hard.csv\n",
      "  Reading: Saccade Amplitude data_jw_harder.csv\n",
      "  Reading: Saccade Amplitude data_jw_moderate.csv\n",
      "  Reading: Saccade Amplitude data_kb_easier.csv\n",
      "  Reading: Saccade Amplitude data_kb_easy.csv\n",
      "  Reading: Saccade Amplitude data_kb_hard.csv\n",
      "  Reading: Saccade Amplitude data_kb_harder.csv\n",
      "  Reading: Saccade Amplitude data_kb_moderate.csv\n",
      "  Reading: Saccade Amplitude data_kw_easier.csv\n",
      "  Reading: Saccade Amplitude data_kw_easy.csv\n",
      "  Reading: Saccade Amplitude data_kw_hard.csv\n",
      "  Reading: Saccade Amplitude data_kw_harder.csv\n",
      "  Reading: Saccade Amplitude data_kw_moderate.csv\n",
      "  Reading: Saccade Amplitude data_mw_easier.csv\n",
      "  Reading: Saccade Amplitude data_mw_easy.csv\n",
      "  Reading: Saccade Amplitude data_mw_hard.csv\n",
      "  Reading: Saccade Amplitude data_mw_harder.csv\n",
      "  Reading: Saccade Amplitude data_mw_moderate.csv\n",
      "  Reading: Saccade Amplitude data_sh_easier.csv\n",
      "  Reading: Saccade Amplitude data_sh_easy.csv\n",
      "  Reading: Saccade Amplitude data_sh_hard.csv\n",
      "  Reading: Saccade Amplitude data_sh_harder.csv\n",
      "  Reading: Saccade Amplitude data_sh_moderate.csv\n",
      "  Reading: Saccade Amplitude data_sm_easier.csv\n",
      "  Reading: Saccade Amplitude data_sm_easy.csv\n",
      "  Reading: Saccade Amplitude data_sm_hard.csv\n",
      "  Reading: Saccade Amplitude data_sm_harder.csv\n",
      "  Reading: Saccade Amplitude data_sm_moderate.csv\n",
      "  Reading: Saccade Amplitude data_ym_easier.csv\n",
      "  Reading: Saccade Amplitude data_ym_easy.csv\n",
      "  Reading: Saccade Amplitude data_ym_hard.csv\n",
      "  Reading: Saccade Amplitude data_ym_harder.csv\n",
      "  Reading: Saccade Amplitude data_ym_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_cy_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_cy_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_cy_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_cy_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_cy_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_dh_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_dh_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_dh_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_dh_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_dh_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_gb_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_gb_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_gb_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_gb_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_gb_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_jj_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_jj_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_jj_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_jj_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_jj_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_jw_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_jw_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_jw_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_jw_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_jw_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_kb_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_kb_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_kb_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_kb_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_kb_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_kw_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_kw_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_kw_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_kw_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_kw_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_mw_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_mw_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_mw_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_mw_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_mw_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_sh_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_sh_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_sh_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_sh_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_sh_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_sm_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_sm_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_sm_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_sm_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_sm_moderate.csv\n",
      "  Reading: Tonic and Phasic GSR data_ym_easier.csv\n",
      "  Reading: Tonic and Phasic GSR data_ym_easy.csv\n",
      "  Reading: Tonic and Phasic GSR data_ym_hard.csv\n",
      "  Reading: Tonic and Phasic GSR data_ym_harder.csv\n",
      "  Reading: Tonic and Phasic GSR data_ym_moderate.csv\n",
      "Total files read: 495\n",
      "\n",
      "==================================================\n",
      "Running 5-class classification with Stratified K-Fold evaluation\n",
      "==================================================\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: No data available after assumed game start\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Warning: 'game start' marker not found. Using all data.\n",
      "Dataset shape: (1508202, 10, 11)\n",
      "Number of sequences: 1508202\n",
      "Sequence length: 10\n",
      "Number of features: 11\n",
      "Class distribution: (array(['easier', 'easy', 'hard', 'harder', 'moderate'], dtype='<U8'), array([350355, 338652, 256016, 232733, 330446], dtype=int64))\n",
      "\n",
      "After Label Encoding:\n",
      "Classes: ['easier' 'easy' 'hard' 'harder' 'moderate']\n",
      "Encoded labels: (array([0, 1, 2, 3, 4]), array([350355, 338652, 256016, 232733, 330446], dtype=int64))\n",
      "\n",
      "Starting Stratified K-Fold evaluation with best hyperparameters...\n",
      "\n",
      "Stratified K-Fold Iteration 1/3\n",
      "Applying SMOTE to the training data...\n",
      "After SMOTE - Training data shape: (1167850, 10, 11), Training labels shape: (1167850,)\n",
      "An error occurred during the kfold evaluation or model saving:\n",
      "y contains previously unseen labels: '0'\n",
      "Traceback:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_encode.py\", line 225, in _encode\n",
      "    return _map_to_integer(values, uniques)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_encode.py\", line 165, in _map_to_integer\n",
      "    return np.array([table[v] for v in values])\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_encode.py\", line 165, in <listcomp>\n",
      "    return np.array([table[v] for v in values])\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_encode.py\", line 159, in __missing__\n",
      "    raise KeyError(key)\n",
      "KeyError: '0'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8372\\143342927.py\", line 379, in main\n",
      "    loso_mean, loso_std = stratified_kfold_evaluation(X, y_encoded, participants, n_classes, best_hyperparameters, le)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8372\\143342927.py\", line 264, in stratified_kfold_evaluation\n",
      "    y_train_cat = to_categorical(label_encoder.transform(y_train_resampled), num_classes=n_classes)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\", line 137, in transform\n",
      "    return _encode(y, uniques=self.classes_)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_encode.py\", line 227, in _encode\n",
      "    raise ValueError(f\"y contains previously unseen labels: {str(e)}\")\n",
      "ValueError: y contains previously unseen labels: '0'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Change csv files to dataframes\n",
    "def read_csv_files(directory):\n",
    "    data_frames = {}\n",
    "    print(\"\\nReading CSV files:\")\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            print(f\"  Reading: {filename}\")\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            parts = filename.split('_')\n",
    "            file_type = parts[0]\n",
    "            participant = parts[-2] if len(parts) > 2 else 'unknown'\n",
    "            difficulty = parts[-1].split('.')[0] if len(parts) > 1 else 'unknown'\n",
    "            if not file_type == 'E4 ACC data':\n",
    "                key = (participant, difficulty, file_type)\n",
    "                data_frames[key] = df\n",
    "            else:\n",
    "                print(f\"    Skipped: {filename} (ACC data)\")\n",
    "    print(f\"Total files read: {len(data_frames)}\")\n",
    "    return data_frames\n",
    "\n",
    "# Select useful data only (after game start, interpolate)\n",
    "def preprocess_data(df, baseline_duration):\n",
    "    if 'started' in df['game started'].values:\n",
    "        start_idx = df[df['game started'] == 'started'].index[0]\n",
    "        start_unix_time = df.loc[start_idx, 'UNIX Time']\n",
    "        target_unix_time = start_unix_time - baseline_duration\n",
    "        idx = (df['UNIX Time'] - target_unix_time).abs().idxmin()\n",
    "        df = df.iloc[idx:]\n",
    "    else:\n",
    "        print(\"Warning: 'game start' marker not found. Using all data.\")\n",
    "        \n",
    "    warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def z_score_normalize(df, baseline_duration):\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.drop('UNIX Time')\n",
    "    baseline = df.iloc[:baseline_duration]\n",
    "    mean = baseline[numeric_columns].mean()\n",
    "    std = baseline[numeric_columns].std()\n",
    "    std = std.replace(0, 1)  # Replace zero std with 1 to avoid division by zero\n",
    "    normalized_df = df.copy()\n",
    "    normalized_df[numeric_columns] = (df[numeric_columns] - mean) / std\n",
    "    return normalized_df\n",
    "\n",
    "def process_data(df, baseline_duration):\n",
    "    df = preprocess_data(df, baseline_duration)\n",
    "    \n",
    "    if 'UNIX Time' not in df.columns or df['UNIX Time'].isnull().all():\n",
    "        print(\"Error: 'UNIX Time' column not found or all values are null\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    game_start_time = df['UNIX Time'].min() + 10\n",
    "    \n",
    "    unix_time = df['UNIX Time']\n",
    "    df_numeric = df.select_dtypes(include=[np.number]).drop(columns=['UNIX Time'])\n",
    "    df_numeric['UNIX Time'] = unix_time\n",
    "    \n",
    "    normalized_df = z_score_normalize(df_numeric, baseline_duration)\n",
    "    \n",
    "    df_after_start = normalized_df[normalized_df['UNIX Time'] >= game_start_time]\n",
    "    \n",
    "    if df_after_start.empty:\n",
    "        print(f\"Warning: No data available after assumed game start\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_numeric = df_after_start.drop(columns=['UNIX Time'])\n",
    "    \n",
    "    return df_numeric\n",
    "\n",
    "def make_sequences(df, window_size):\n",
    "    sequences = []\n",
    "    for i in range(0, len(df) - window_size + 1):\n",
    "        sequences.append(df.iloc[i:i+window_size].values)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def create_dataset(data_frames, window_size, expected_features, baseline_duration):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    participant_data = []\n",
    "    all_columns = set()\n",
    "    \n",
    "    for (participant, difficulty, file_type), df in data_frames.items():\n",
    "        processed_df = process_data(df, baseline_duration)\n",
    "        all_columns.update(processed_df.columns)\n",
    "    \n",
    "    all_columns = sorted(list(all_columns))[:expected_features]  # Limit to expected number of features\n",
    "    \n",
    "    for (participant, difficulty, file_type), df in data_frames.items():\n",
    "        processed_df = process_data(df, baseline_duration)\n",
    "        \n",
    "        for col in all_columns:\n",
    "            if col not in processed_df.columns:\n",
    "                processed_df[col] = 0\n",
    "        \n",
    "        processed_df = processed_df[all_columns]\n",
    "        \n",
    "        # Pad or truncate to match expected features\n",
    "        if processed_df.shape[1] < expected_features:\n",
    "            padding = pd.DataFrame(0, index=processed_df.index, \n",
    "                                   columns=[f'pad_{i}' for i in range(expected_features - processed_df.shape[1])])\n",
    "            processed_df = pd.concat([processed_df, padding], axis=1)\n",
    "        elif processed_df.shape[1] > expected_features:\n",
    "            processed_df = processed_df.iloc[:, :expected_features]\n",
    "            \n",
    "        sequences = make_sequences(processed_df, window_size)\n",
    "        if len(sequences) > 0:\n",
    "            X_data.append(sequences)\n",
    "            y_data.extend([difficulty] * len(sequences))\n",
    "            participant_data.extend([participant] * len(sequences))\n",
    "    \n",
    "    X = np.concatenate(X_data)\n",
    "    y = np.array(y_data)\n",
    "    participants = np.array(participant_data)\n",
    "    \n",
    "    return X, y, participants\n",
    "\n",
    "def create_advanced_lstm_model(input_shape, n_classes, lstm_units, dense_units, dropout_rate):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    low_level = TimeDistributed(Dense(32, activation='relu'))(inputs)\n",
    "    low_level = TimeDistributed(Dense(16, activation='relu'))(low_level)\n",
    "    \n",
    "    high_level = LSTM(lstm_units[0], return_sequences=True)(inputs)\n",
    "    high_level = LSTM(lstm_units[1])(high_level)\n",
    "    \n",
    "    combined = Concatenate()([tf.keras.layers.Flatten()(low_level), high_level])\n",
    "    \n",
    "    x = Dense(dense_units, activation='relu')(combined)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    outputs = Dense(n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def loso_evaluation(X, y, participants, n_classes, hyperparameters):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    \n",
    "    loso_scores = []\n",
    "    for i, (train_index, test_index) in enumerate(logo.split(X, y, participants)):\n",
    "        print(f\"\\nLOSO Iteration {i+1}/{len(np.unique(participants))}\")\n",
    "        print(f\"Test participant: {participants[test_index[0]]}\")\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Validation split from the train set\n",
    "        val_split_idx = int(0.8 * len(X_train))  # 80% train, 20% validation\n",
    "        X_train, X_val = X_train[:val_split_idx], X_train[val_split_idx:]\n",
    "        y_train, y_val = y_train[:val_split_idx], y_train[val_split_idx:]\n",
    "        \n",
    "        model = create_advanced_lstm_model(\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "            n_classes=n_classes,\n",
    "            lstm_units=hyperparameters['lstm_units'],\n",
    "            dense_units=hyperparameters['dense_units'],\n",
    "            dropout_rate=hyperparameters['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        y_train_cat = to_categorical(le.fit_transform(y_train))\n",
    "        y_val_cat = to_categorical(le.transform(y_val))\n",
    "        y_test_cat = to_categorical(le.transform(y_test))\n",
    "        \n",
    "        # EarlyStopping 콜백 추가\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(X_train, \n",
    "                            y_train_cat,\n",
    "                            epochs=hyperparameters['epochs'], \n",
    "                            batch_size=hyperparameters['batch_size'],\n",
    "                            validation_data=(X_val, y_val_cat),\n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=0)\n",
    "        \n",
    "        # 학습 그래프 그리기\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss 그래프\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy 그래프\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Evaluating model...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "        loso_scores.append(accuracy)\n",
    "        print(f\"Accuracy for participant {participants[test_index[0]]}: {accuracy:.4f}\")\n",
    "        \n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test_classes, y_pred_classes, target_names=le.classes_))\n",
    "        \n",
    "        cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f'Confusion Matrix for participant {participants[test_index[0]]}')\n",
    "        plt.show()\n",
    "    \n",
    "    return np.mean(loso_scores), np.std(loso_scores)\n",
    "\n",
    "def stratified_kfold_evaluation(X, y, n_classes, hyperparameters, label_encoder):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nStratified K-Fold Iteration {i+1}/{skf.n_splits}\")\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Apply SMOTE only on the training data\n",
    "        print(\"Applying SMOTE to the training data...\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flat, y_train)\n",
    "        X_train_resampled = X_train_resampled.reshape(-1, X_train.shape[1], X_train.shape[2])\n",
    "        print(f\"After SMOTE - Training data shape: {X_train_resampled.shape}, Training labels shape: {y_train_resampled.shape}\")\n",
    "        \n",
    "        # Encode labels (using the pre-fitted encoder)\n",
    "        y_train_cat = to_categorical(label_encoder.transform(y_train_resampled), num_classes=n_classes)\n",
    "        y_test_cat = to_categorical(label_encoder.transform(y_test), num_classes=n_classes)\n",
    "        \n",
    "        # EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = create_advanced_lstm_model(\n",
    "            input_shape=(X_train_resampled.shape[1], X_train_resampled.shape[2]),\n",
    "            n_classes=n_classes,\n",
    "            lstm_units=hyperparameters['lstm_units'],\n",
    "            dense_units=hyperparameters['dense_units'],\n",
    "            dropout_rate=hyperparameters['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            X_train_resampled, y_train_cat,\n",
    "            epochs=hyperparameters['epochs'],\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            validation_data=(X_test, y_test_cat),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 1 for more detailed output\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Fold {i+1} - Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Fold {i+1} - Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        print(\"Evaluating model on validation data...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "        fold_accuracies.append(accuracy)\n",
    "        print(f\"Accuracy for fold {i+1}: {accuracy:.4f}\")\n",
    "        \n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))\n",
    "        \n",
    "        cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f'Confusion Matrix for fold {i+1}')\n",
    "        plt.show()\n",
    "    \n",
    "    return np.mean(fold_accuracies), np.std(fold_accuracies)\n",
    "\n",
    "def main():\n",
    "    directory = r'C:\\Users\\USER\\Downloads\\pilotdata'\n",
    "    data_frames = read_csv_files(directory)\n",
    "    \n",
    "    window_size = 10  # 10-second window\n",
    "    n_classes = 5  # Only 5-class classification\n",
    "    expected_features = 11  # Set this to the number of features you expect\n",
    "    baseline_duration = 30  # Set this to your baseline duration in seconds\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running 5-class classification with Stratified K-Fold evaluation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    X, y, participants = create_dataset(data_frames, window_size, expected_features, baseline_duration)\n",
    "    \n",
    "    if X is None or y is None or participants is None:\n",
    "        print(\"Failed to create dataset.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Number of sequences: {X.shape[0]}\")\n",
    "    print(f\"Sequence length: {X.shape[1]}\")\n",
    "    print(f\"Number of features: {X.shape[2]}\")\n",
    "    print(f\"Class distribution: {np.unique(y, return_counts=True)}\")\n",
    "    \n",
    "    # Encode labels once\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)  # Fit on the entire dataset to ensure consistency\n",
    "    y_encoded = le.transform(y)\n",
    "    \n",
    "    print(\"\\nAfter Label Encoding:\")\n",
    "    print(f\"Classes: {le.classes_}\")\n",
    "    print(f\"Encoded labels: {np.unique(y_encoded, return_counts=True)}\")\n",
    "    \n",
    "    # Use the best hyperparameters you provided\n",
    "    best_hyperparameters = {\n",
    "        'lstm_units': (32, 16),\n",
    "        'dense_units': 64,\n",
    "        'dropout_rate': 0.3,\n",
    "        'epochs': 50,\n",
    "        'batch_size': 256\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStarting Stratified K-Fold evaluation with best hyperparameters...\")\n",
    "        loso_mean, loso_std = stratified_kfold_evaluation(X, y_encoded, n_classes, best_hyperparameters, le)\n",
    "        print(f\"\\nStratified K-Fold evaluation results:\")\n",
    "        print(f\"Mean accuracy: {loso_mean:.4f} (+/- {loso_std:.4f})\")\n",
    "        \n",
    "        # Optionally, train a final model on the entire dataset\n",
    "        # Apply SMOTE to the entire dataset (if desired), but be cautious of data leakage\n",
    "        # Here, we'll skip SMOTE for the final model to avoid potential leakage\n",
    "        final_model = create_advanced_lstm_model(\n",
    "            input_shape=(X.shape[1], X.shape[2]),\n",
    "            n_classes=n_classes,\n",
    "            lstm_units=best_hyperparameters['lstm_units'],\n",
    "            dense_units=best_hyperparameters['dense_units'],\n",
    "            dropout_rate=best_hyperparameters['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        y_cat = to_categorical(y_encoded, num_classes=n_classes)\n",
    "        \n",
    "        print(\"\\nTraining final model on all data...\")\n",
    "        early_stopping_final = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "        final_model.fit(X, y_cat, epochs=best_hyperparameters['epochs'], \n",
    "                       batch_size=best_hyperparameters['batch_size'], \n",
    "                       callbacks=[early_stopping_final], verbose=1)\n",
    "        \n",
    "        # Save the final model\n",
    "        model_filename = r\"C:\\Users\\HCIS\\Downloads\\pilotdata\\best_lstm_model_lowhigh_3class0723.h5\"\n",
    "        final_model.save(model_filename)\n",
    "        print(f\"\\nFinal model saved as {model_filename}\")\n",
    "        \n",
    "        # Save the label encoder\n",
    "        le_filename = r\"C:\\Users\\HCIS\\Downloads\\pilotdata\\label_encoder_lowhigh0723.joblib\"\n",
    "        joblib.dump(le, le_filename)\n",
    "        print(f\"Label encoder saved as {le_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An error occurred during the kfold evaluation or model saving:\")\n",
    "        print(str(e))\n",
    "        print(\"Traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
