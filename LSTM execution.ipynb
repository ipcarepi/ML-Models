{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV files:\n",
      "  Reading: annotation_cy_1.csv\n",
      "  Reading: annotation_cy_2.csv\n",
      "  Reading: annotation_cy_3.csv\n",
      "  Reading: annotation_cy_4.csv\n",
      "  Reading: annotation_cy_5.csv\n",
      "  Reading: annotation_db_1.csv\n",
      "  Reading: annotation_db_2.csv\n",
      "  Reading: annotation_db_3.csv\n",
      "  Reading: annotation_db_4.csv\n",
      "  Reading: annotation_db_5.csv\n",
      "  Reading: annotation_es_1.csv\n",
      "  Reading: annotation_es_2.csv\n",
      "  Reading: annotation_es_3.csv\n",
      "  Reading: annotation_es_4.csv\n",
      "  Reading: annotation_es_5.csv\n",
      "  Reading: annotation_gb_1.csv\n",
      "  Reading: annotation_gb_2.csv\n",
      "  Reading: annotation_gb_3.csv\n",
      "  Reading: annotation_gb_4.csv\n",
      "  Reading: annotation_gb_5.csv\n",
      "  Reading: annotation_jw_1.csv\n",
      "  Reading: annotation_jw_2.csv\n",
      "  Reading: annotation_jw_3.csv\n",
      "  Reading: annotation_jw_4.csv\n",
      "  Reading: annotation_jw_5.csv\n",
      "  Reading: annotation_kb_1.csv\n",
      "  Reading: annotation_kb_2.csv\n",
      "  Reading: annotation_kb_3.csv\n",
      "  Reading: annotation_kb_4.csv\n",
      "  Reading: annotation_kb_5.csv\n",
      "  Reading: annotation_mw_1.csv\n",
      "  Reading: annotation_mw_2.csv\n",
      "  Reading: annotation_mw_3.csv\n",
      "  Reading: annotation_mw_4.csv\n",
      "  Reading: annotation_mw_5.csv\n",
      "  Reading: annotation_sh_1.csv\n",
      "  Reading: annotation_sh_2.csv\n",
      "  Reading: annotation_sh_3.csv\n",
      "  Reading: annotation_sh_4.csv\n",
      "  Reading: annotation_sh_5.csv\n",
      "  Reading: annotation_sm_1.csv\n",
      "  Reading: annotation_sm_2.csv\n",
      "  Reading: annotation_sm_3.csv\n",
      "  Reading: annotation_sm_4.csv\n",
      "  Reading: annotation_sm_5.csv\n",
      "  Reading: annotation_ym_1.csv\n",
      "  Reading: annotation_ym_2.csv\n",
      "  Reading: annotation_ym_3.csv\n",
      "  Reading: annotation_ym_4.csv\n",
      "  Reading: annotation_ym_5.csv\n",
      "  Reading: E4 BVP, IBI data_cy_1.csv\n",
      "  Reading: E4 BVP, IBI data_cy_2.csv\n",
      "  Reading: E4 BVP, IBI data_cy_3.csv\n",
      "  Reading: E4 BVP, IBI data_cy_4.csv\n",
      "  Reading: E4 BVP, IBI data_cy_5.csv\n",
      "  Reading: E4 BVP, IBI data_db_1.csv\n",
      "  Reading: E4 BVP, IBI data_db_2.csv\n",
      "  Reading: E4 BVP, IBI data_db_3.csv\n",
      "  Reading: E4 BVP, IBI data_db_4.csv\n",
      "  Reading: E4 BVP, IBI data_db_5.csv\n",
      "  Reading: E4 BVP, IBI data_es_1.csv\n",
      "  Reading: E4 BVP, IBI data_es_2.csv\n",
      "  Reading: E4 BVP, IBI data_es_3.csv\n",
      "  Reading: E4 BVP, IBI data_es_4.csv\n",
      "  Reading: E4 BVP, IBI data_es_5.csv\n",
      "  Reading: E4 BVP, IBI data_gb_1.csv\n",
      "  Reading: E4 BVP, IBI data_gb_2.csv\n",
      "  Reading: E4 BVP, IBI data_gb_3.csv\n",
      "  Reading: E4 BVP, IBI data_gb_4.csv\n",
      "  Reading: E4 BVP, IBI data_gb_5.csv\n",
      "  Reading: E4 BVP, IBI data_jw_1.csv\n",
      "  Reading: E4 BVP, IBI data_jw_2.csv\n",
      "  Reading: E4 BVP, IBI data_jw_3.csv\n",
      "  Reading: E4 BVP, IBI data_jw_4.csv\n",
      "  Reading: E4 BVP, IBI data_jw_5.csv\n",
      "  Reading: E4 BVP, IBI data_kb_1.csv\n",
      "  Reading: E4 BVP, IBI data_kb_2.csv\n",
      "  Reading: E4 BVP, IBI data_kb_3.csv\n",
      "  Reading: E4 BVP, IBI data_kb_4.csv\n",
      "  Reading: E4 BVP, IBI data_kb_5.csv\n",
      "  Reading: E4 BVP, IBI data_mw_1.csv\n",
      "  Reading: E4 BVP, IBI data_mw_2.csv\n",
      "  Reading: E4 BVP, IBI data_mw_3.csv\n",
      "  Reading: E4 BVP, IBI data_mw_4.csv\n",
      "  Reading: E4 BVP, IBI data_mw_5.csv\n",
      "  Reading: E4 BVP, IBI data_sh_1.csv\n",
      "  Reading: E4 BVP, IBI data_sh_2.csv\n",
      "  Reading: E4 BVP, IBI data_sh_3.csv\n",
      "  Reading: E4 BVP, IBI data_sh_4.csv\n",
      "  Reading: E4 BVP, IBI data_sh_5.csv\n",
      "  Reading: E4 BVP, IBI data_sm_1.csv\n",
      "  Reading: E4 BVP, IBI data_sm_2.csv\n",
      "  Reading: E4 BVP, IBI data_sm_3.csv\n",
      "  Reading: E4 BVP, IBI data_sm_4.csv\n",
      "  Reading: E4 BVP, IBI data_sm_5.csv\n",
      "  Reading: E4 BVP, IBI data_ym_1.csv\n",
      "  Reading: E4 BVP, IBI data_ym_2.csv\n",
      "  Reading: E4 BVP, IBI data_ym_3.csv\n",
      "  Reading: E4 BVP, IBI data_ym_4.csv\n",
      "  Reading: E4 BVP, IBI data_ym_5.csv\n",
      "  Reading: E4 GSR, TMP data_cy_1.csv\n",
      "  Reading: E4 GSR, TMP data_cy_2.csv\n",
      "  Reading: E4 GSR, TMP data_cy_3.csv\n",
      "  Reading: E4 GSR, TMP data_cy_4.csv\n",
      "  Reading: E4 GSR, TMP data_cy_5.csv\n",
      "  Reading: E4 GSR, TMP data_db_1.csv\n",
      "  Reading: E4 GSR, TMP data_db_2.csv\n",
      "  Reading: E4 GSR, TMP data_db_3.csv\n",
      "  Reading: E4 GSR, TMP data_db_4.csv\n",
      "  Reading: E4 GSR, TMP data_db_5.csv\n",
      "  Reading: E4 GSR, TMP data_es_1.csv\n",
      "  Reading: E4 GSR, TMP data_es_2.csv\n",
      "  Reading: E4 GSR, TMP data_es_3.csv\n",
      "  Reading: E4 GSR, TMP data_es_4.csv\n",
      "  Reading: E4 GSR, TMP data_es_5.csv\n",
      "  Reading: E4 GSR, TMP data_gb_1.csv\n",
      "  Reading: E4 GSR, TMP data_gb_2.csv\n",
      "  Reading: E4 GSR, TMP data_gb_3.csv\n",
      "  Reading: E4 GSR, TMP data_gb_4.csv\n",
      "  Reading: E4 GSR, TMP data_gb_5.csv\n",
      "  Reading: E4 GSR, TMP data_jw_1.csv\n",
      "  Reading: E4 GSR, TMP data_jw_2.csv\n",
      "  Reading: E4 GSR, TMP data_jw_3.csv\n",
      "  Reading: E4 GSR, TMP data_jw_4.csv\n",
      "  Reading: E4 GSR, TMP data_jw_5.csv\n",
      "  Reading: E4 GSR, TMP data_kb_1.csv\n",
      "  Reading: E4 GSR, TMP data_kb_2.csv\n",
      "  Reading: E4 GSR, TMP data_kb_3.csv\n",
      "  Reading: E4 GSR, TMP data_kb_4.csv\n",
      "  Reading: E4 GSR, TMP data_kb_5.csv\n",
      "  Reading: E4 GSR, TMP data_mw_1.csv\n",
      "  Reading: E4 GSR, TMP data_mw_2.csv\n",
      "  Reading: E4 GSR, TMP data_mw_3.csv\n",
      "  Reading: E4 GSR, TMP data_mw_4.csv\n",
      "  Reading: E4 GSR, TMP data_mw_5.csv\n",
      "  Reading: E4 GSR, TMP data_sh_1.csv\n",
      "  Reading: E4 GSR, TMP data_sh_2.csv\n",
      "  Reading: E4 GSR, TMP data_sh_3.csv\n",
      "  Reading: E4 GSR, TMP data_sh_4.csv\n",
      "  Reading: E4 GSR, TMP data_sh_5.csv\n",
      "  Reading: E4 GSR, TMP data_sm_1.csv\n",
      "  Reading: E4 GSR, TMP data_sm_2.csv\n",
      "  Reading: E4 GSR, TMP data_sm_3.csv\n",
      "  Reading: E4 GSR, TMP data_sm_4.csv\n",
      "  Reading: E4 GSR, TMP data_sm_5.csv\n",
      "  Reading: E4 GSR, TMP data_ym_1.csv\n",
      "  Reading: E4 GSR, TMP data_ym_2.csv\n",
      "  Reading: E4 GSR, TMP data_ym_3.csv\n",
      "  Reading: E4 GSR, TMP data_ym_4.csv\n",
      "  Reading: E4 GSR, TMP data_ym_5.csv\n",
      "  Reading: Eye Feature data_cy_1.csv\n",
      "  Reading: Eye Feature data_cy_2.csv\n",
      "  Reading: Eye Feature data_cy_3.csv\n",
      "  Reading: Eye Feature data_cy_4.csv\n",
      "  Reading: Eye Feature data_cy_5.csv\n",
      "  Reading: Eye Feature data_db_1.csv\n",
      "  Reading: Eye Feature data_db_2.csv\n",
      "  Reading: Eye Feature data_db_3.csv\n",
      "  Reading: Eye Feature data_db_4.csv\n",
      "  Reading: Eye Feature data_db_5.csv\n",
      "  Reading: Eye Feature data_es_1.csv\n",
      "  Reading: Eye Feature data_es_2.csv\n",
      "  Reading: Eye Feature data_es_3.csv\n",
      "  Reading: Eye Feature data_es_4.csv\n",
      "  Reading: Eye Feature data_es_5.csv\n",
      "  Reading: Eye Feature data_gb_1.csv\n",
      "  Reading: Eye Feature data_gb_2.csv\n",
      "  Reading: Eye Feature data_gb_3.csv\n",
      "  Reading: Eye Feature data_gb_4.csv\n",
      "  Reading: Eye Feature data_gb_5.csv\n",
      "  Reading: Eye Feature data_jw_1.csv\n",
      "  Reading: Eye Feature data_jw_2.csv\n",
      "  Reading: Eye Feature data_jw_3.csv\n",
      "  Reading: Eye Feature data_jw_4.csv\n",
      "  Reading: Eye Feature data_jw_5.csv\n",
      "  Reading: Eye Feature data_kb_1.csv\n",
      "  Reading: Eye Feature data_kb_2.csv\n",
      "  Reading: Eye Feature data_kb_3.csv\n",
      "  Reading: Eye Feature data_kb_4.csv\n",
      "  Reading: Eye Feature data_kb_5.csv\n",
      "  Reading: Eye Feature data_mw_1.csv\n",
      "  Reading: Eye Feature data_mw_2.csv\n",
      "  Reading: Eye Feature data_mw_3.csv\n",
      "  Reading: Eye Feature data_mw_4.csv\n",
      "  Reading: Eye Feature data_mw_5.csv\n",
      "  Reading: Eye Feature data_sh_1.csv\n",
      "  Reading: Eye Feature data_sh_2.csv\n",
      "  Reading: Eye Feature data_sh_3.csv\n",
      "  Reading: Eye Feature data_sh_4.csv\n",
      "  Reading: Eye Feature data_sh_5.csv\n",
      "  Reading: Eye Feature data_sm_1.csv\n",
      "  Reading: Eye Feature data_sm_2.csv\n",
      "  Reading: Eye Feature data_sm_3.csv\n",
      "  Reading: Eye Feature data_sm_4.csv\n",
      "  Reading: Eye Feature data_sm_5.csv\n",
      "  Reading: Eye Feature data_ym_1.csv\n",
      "  Reading: Eye Feature data_ym_2.csv\n",
      "  Reading: Eye Feature data_ym_3.csv\n",
      "  Reading: Eye Feature data_ym_4.csv\n",
      "  Reading: Eye Feature data_ym_5.csv\n",
      "  Reading: Eye Sensor data_cy_1.csv\n",
      "  Reading: Eye Sensor data_cy_2.csv\n",
      "  Reading: Eye Sensor data_cy_3.csv\n",
      "  Reading: Eye Sensor data_cy_4.csv\n",
      "  Reading: Eye Sensor data_cy_5.csv\n",
      "  Reading: Eye Sensor data_db_1.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 546\u001b[0m\n\u001b[0;32m    543\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 546\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 450\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    449\u001b[0m     directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUSER\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpilotdata\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 450\u001b[0m     data_frames, annotations \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# 10-second window\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Only 5-class classification\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mread_csv_files\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Reading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     parts \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m     file_type \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:625\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m--> 625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1996\u001b[0m, in \u001b[0;36mTextFileReader.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\n\u001b[0;32m   1991\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1992\u001b[0m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1993\u001b[0m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1994\u001b[0m     traceback: TracebackType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1995\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1996\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1624\u001b[0m, in \u001b[0;36mTextFileReader.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:144\u001b[0m, in \u001b[0;36mIOHandles.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles:\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Change csv files to dataframes\n",
    "def read_csv_files(directory):\n",
    "    data_frames = {}\n",
    "    annotations = {}\n",
    "    print(\"\\nReading CSV files:\")\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            print(f\"  Reading: {filename}\")\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            parts = filename.split('_')\n",
    "            file_type = parts[0]\n",
    "            participant = parts[-2] if len(parts) > 2 else 'unknown'\n",
    "            session = parts[-1].split('.')[0] if len(parts) > 1 else 'unknown'\n",
    "            if file_type == 'annotation':\n",
    "                key = (participant, session)\n",
    "                annotations[key] = df\n",
    "            elif not file_type == 'E4 ACC data':\n",
    "                key = (participant, session, file_type)\n",
    "                data_frames[key] = df\n",
    "            else:\n",
    "                print(f\"    Skipped: {filename} (ACC data)\")\n",
    "    print(f\"Total files read: {len(data_frames)}\")\n",
    "    return data_frames, annotations\n",
    "\n",
    "# Select useful data only (after game start, interpolate)\n",
    "def preprocess_data(df, baseline_duration):\n",
    "    if 'started' in df['game started'].values:\n",
    "        start_idx = df[df['game started'] == 'started'].index[0]\n",
    "        start_unix_time = df.loc[start_idx, 'UNIX Time']\n",
    "        target_unix_time = start_unix_time - baseline_duration\n",
    "        idx = (df['UNIX Time'] - target_unix_time).abs().idxmin()\n",
    "        df = df.iloc[idx:]\n",
    "    else:\n",
    "        print(\"Warning: 'game start' marker not found. Using all data.\")\n",
    "        \n",
    "    warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def z_score_normalize(df, baseline_duration):\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.drop('UNIX Time')\n",
    "    baseline = df.iloc[:baseline_duration]\n",
    "    mean = baseline[numeric_columns].mean()\n",
    "    std = baseline[numeric_columns].std()\n",
    "    std = std.replace(0, 1)  # Replace zero std with 1 to avoid division by zero\n",
    "    normalized_df = df.copy()\n",
    "    normalized_df[numeric_columns] = (df[numeric_columns] - mean) / std\n",
    "    return normalized_df\n",
    "\n",
    "def process_data(df, baseline_duration):\n",
    "    df = preprocess_data(df, baseline_duration)\n",
    "    \n",
    "    if 'UNIX Time' not in df.columns or df['UNIX Time'].isnull().all():\n",
    "        print(\"Error: 'UNIX Time' column not found or all values are null\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    game_start_time = df['UNIX Time'].min() + 5\n",
    "    \n",
    "    unix_time = df['UNIX Time']\n",
    "    df_numeric = df.select_dtypes(include=[np.number]).drop(columns=['UNIX Time'])\n",
    "    df_numeric['UNIX Time'] = unix_time\n",
    "    \n",
    "    normalized_df = z_score_normalize(df_numeric, baseline_duration)\n",
    "    \n",
    "    df_after_start = normalized_df[normalized_df['UNIX Time'] >= game_start_time]\n",
    "    \n",
    "    if df_after_start.empty:\n",
    "        print(f\"Warning: No data available after assumed game start\")\n",
    "        return pd.DataFrame(columns=['UNIX Time'])\n",
    "    \n",
    "    return df_after_start\n",
    "\n",
    "def make_sequences(df, time_df, annotation, window_size):\n",
    "    sequences = []\n",
    "    challenge_sequences = []\n",
    "    engagement_sequences = []\n",
    "    for i in range(0, len(df) - window_size + 1):\n",
    "        sequences.append(df.iloc[i:i+window_size].values)\n",
    "        time = time_df.iloc[i]\n",
    "        for j in range(0, len(annotation)):\n",
    "            if annotation.loc[j, 'UNIX Start Time'] <= time and annotation.loc[j, 'UNIX End Time'] >= time:\n",
    "                challenge_sequences.append(annotation.loc[j, 'Challenge'])\n",
    "                engagement_sequences.append(annotation.loc[j, 'Engagement'])\n",
    "                break\n",
    "        else:\n",
    "            challenge_sequences.append(1)\n",
    "            engagement_sequences.append(1)\n",
    "    return np.array(sequences), np.array(challenge_sequences), np.array(engagement_sequences)\n",
    "\n",
    "def create_dataset(data_frames, annotations, window_size, expected_features, baseline_duration):\n",
    "    X_data = []\n",
    "    challenge_data = []\n",
    "    engagement_data = []\n",
    "    participant_data = []\n",
    "    all_columns = set()\n",
    "    \n",
    "    for (participant, session, file_type), df in data_frames.items():\n",
    "        processed_df = process_data(df, baseline_duration)\n",
    "        processed_df = processed_df.drop(columns=['UNIX Time'])\n",
    "        all_columns.update(processed_df.columns)\n",
    "        \n",
    "    all_columns.discard('game started')\n",
    "    print(f'all_columns = {all_columns}')\n",
    "    print(f'length of all_columns = {len(all_columns)}')\n",
    "    \n",
    "    all_columns = sorted(list(all_columns))[:expected_features] # Limit to expected number of features\n",
    "    \n",
    "    for (participant, session, file_type), df in data_frames.items():\n",
    "        processed_df_unix = process_data(df, baseline_duration)\n",
    "        processed_df = processed_df_unix.drop(columns=['UNIX Time'])\n",
    "        \n",
    "        for col in all_columns:\n",
    "            if col not in processed_df.columns:\n",
    "                processed_df[col] = 0\n",
    "        \n",
    "        processed_df = processed_df[all_columns]\n",
    "        \n",
    "        # Pad or truncate to match expected features\n",
    "        if processed_df.shape[1] < expected_features:\n",
    "            padding = pd.DataFrame(0, index=processed_df.index, \n",
    "                                   columns=[f'pad_{i}' for i in range(expected_features - processed_df.shape[1])])\n",
    "            processed_df = pd.concat([processed_df, padding], axis=1)\n",
    "        elif processed_df.shape[1] > expected_features:\n",
    "            processed_df = processed_df.iloc[:, :expected_features]\n",
    "            \n",
    "        annotation = annotations[(participant, session)]\n",
    "        print(participant, session)\n",
    "        sequences, challenge_sequences, engagement_sequences = make_sequences(processed_df, processed_df_unix['UNIX Time'], annotation, window_size)\n",
    "        if len(sequences) > 0:\n",
    "            X_data.append(sequences)\n",
    "            challenge_data.append(challenge_sequences)\n",
    "            engagement_data.append(engagement_sequences)\n",
    "            participant_data.extend([participant] * len(sequences))\n",
    "    \n",
    "    X = np.concatenate(X_data)\n",
    "    challenge = np.concatenate(challenge_data)\n",
    "    engagement = np.concatenate(engagement_data)\n",
    "    participants = np.array(participant_data)\n",
    "    \n",
    "    return X, challenge, engagement, participants\n",
    "\n",
    "def create_advanced_lstm_model(input_shape, n_classes, lstm_units, dense_units, dropout_rate):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    low_level = TimeDistributed(Dense(32, activation='relu'))(inputs)\n",
    "    low_level = TimeDistributed(Dense(16, activation='relu'))(low_level)\n",
    "    \n",
    "    high_level = LSTM(lstm_units[0], return_sequences=True)(inputs)\n",
    "    high_level = LSTM(lstm_units[1])(high_level)\n",
    "    \n",
    "    combined = Concatenate()([tf.keras.layers.Flatten()(low_level), high_level])\n",
    "    \n",
    "    x = Dense(dense_units, activation='relu')(combined)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    outputs = Dense(n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_multi_output_model_with_levels(input_shape, lstm_units, dense_units, dropout_rate):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Low-level 특징 추출\n",
    "    low_level = TimeDistributed(Dense(32, activation='relu'))(inputs)\n",
    "    low_level = TimeDistributed(Dense(16, activation='relu'))(low_level)\n",
    "\n",
    "    # High-level 특징 추출\n",
    "    high_level = LSTM(lstm_units[0], return_sequences=True)(inputs)\n",
    "    high_level = LSTM(lstm_units[1])(high_level)\n",
    "\n",
    "    # Low-level과 High-level 결합\n",
    "    combined = Concatenate()([tf.keras.layers.Flatten()(low_level), high_level])\n",
    "\n",
    "    # Engagement 출력\n",
    "    engagement_output = Dense(dense_units, activation='relu')(combined)\n",
    "    engagement_output = Dropout(dropout_rate)(engagement_output)\n",
    "    engagement_output = Dense(5, activation='softmax', name='engagement_output')(engagement_output)\n",
    "\n",
    "    # Challenge 출력\n",
    "    challenge_output = Dense(dense_units, activation='relu')(combined)\n",
    "    challenge_output = Dropout(dropout_rate)(challenge_output)\n",
    "    challenge_output = Dense(5, activation='softmax', name='challenge_output')(challenge_output)\n",
    "\n",
    "    # 모델 생성\n",
    "    model = Model(inputs=inputs, outputs=[engagement_output, challenge_output])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss={'engagement_output': 'categorical_crossentropy',\n",
    "                        'challenge_output': 'categorical_crossentropy'},\n",
    "                  metrics={'engagement_output': 'accuracy',\n",
    "                           'challenge_output': 'accuracy'})\n",
    "    \n",
    "    return model\n",
    "\n",
    "def loso_evaluation(X, y, participants, hyperparameters):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    loso_scores = []\n",
    "    \n",
    "    y = np.stack((y[0], y[1]), axis=1)\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(logo.split(X, y, participants)):\n",
    "        print(f\"\\nLOSO Iteration {i+1}/{len(np.unique(participants))}\")\n",
    "        print(f\"Test participant: {participants[test_index[0]]}\")\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Validation split from the train set\n",
    "        val_split_idx = int(0.8 * len(X_train))  # 80% train, 20% validation\n",
    "        X_train, X_val = X_train[:val_split_idx], X_train[val_split_idx:]\n",
    "        y_train, y_val = y_train[:val_split_idx], y_train[val_split_idx:]\n",
    "        \n",
    "        model = create_multi_output_model_with_levels(\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "            lstm_units=hyperparameters['lstm_units'],\n",
    "            dense_units=hyperparameters['dense_units'],\n",
    "            dropout_rate=hyperparameters['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        y_train_engagement = to_categorical(y_train[:, 0])\n",
    "        y_train_challenge = to_categorical(y_train[:, 1])\n",
    "        \n",
    "        y_val_engagement = to_categorical(y_val[:, 0])\n",
    "        y_val_challenge = to_categorical(y_val[:, 1])\n",
    "        \n",
    "        y_test_engagement = to_categorical(y_test[:, 0])\n",
    "        y_test_challenge = to_categorical(y_test[:, 1])\n",
    "        \n",
    "        # EarlyStopping 콜백 추가\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(X_train, \n",
    "                            [y_train_engagement, y_train_challenge],\n",
    "                            epochs=hyperparameters['epochs'], \n",
    "                            batch_size=hyperparameters['batch_size'],\n",
    "                            validation_data=(X_val, [y_val_engagement, y_val_challenge]),\n",
    "                            callbacks=[early_stopping], \n",
    "                            verbose=0)\n",
    "        \n",
    "        # 학습 그래프 그리기\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss 그래프\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['engagement_output_loss'], label='Engagement Loss')\n",
    "        plt.plot(history.history['challenge_output_loss'], label='Challenge Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy 그래프\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['engagement_output_accuracy'], label='Engagement Accuracy')\n",
    "        plt.plot(history.history['challenge_output_accuracy'], label='Challenge Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Evaluating model...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_engagement = np.argmax(y_pred[0], axis=1)\n",
    "        y_pred_challenge = np.argmax(y_pred[1], axis=1)\n",
    "        \n",
    "        y_test_engagement_classes = np.argmax(y_test_engagement, axis=1)\n",
    "        y_test_challenge_classes = np.argmax(y_test_challenge, axis=1)\n",
    "        \n",
    "        accuracy_engagement = accuracy_score(y_test_engagement_classes, y_pred_engagement)\n",
    "        accuracy_challenge = accuracy_score(y_test_challenge_classes, y_pred_challenge)\n",
    "        loso_scores.append((accuracy_engagement + accuracy_challenge) / 2)\n",
    "        print(f\"Accuracy for participant {participants[test_index[0]]}:\")\n",
    "        print(f\" - Engagement: {accuracy_engagement:.4f}\")\n",
    "        print(f\" - Challenge: {accuracy_challenge:.4f}\")\n",
    "        \n",
    "        print(\"Classification Report for Engagement Output:\")\n",
    "        print(classification_report(y_test_engagement_classes, y_pred_engagement, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']))\n",
    "        \n",
    "        print(\"Classification Report for Challenge Output:\")\n",
    "        print(classification_report(y_test_challenge_classes, y_pred_challenge, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']))\n",
    "        \n",
    "        # Confusion Matrix for Engagement\n",
    "        cm_engagement = confusion_matrix(y_test_engagement_classes, y_pred_engagement)\n",
    "        disp_engagement = ConfusionMatrixDisplay(confusion_matrix=cm_engagement, display_labels=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'])\n",
    "        disp_engagement.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f'Confusion Matrix for Engagement Output (Participant {participants[test_index[0]]})')\n",
    "        plt.show()\n",
    "\n",
    "        # Confusion Matrix for Challenge\n",
    "        cm_challenge = confusion_matrix(y_test_challenge_classes, y_pred_challenge)\n",
    "        disp_challenge = ConfusionMatrixDisplay(confusion_matrix=cm_challenge, display_labels=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'])\n",
    "        disp_challenge.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f'Confusion Matrix for Challenge Output (Participant {participants[test_index[0]]})')\n",
    "        plt.show()\n",
    "    \n",
    "    return np.mean(loso_scores), np.std(loso_scores)\n",
    "\n",
    "def stratified_kfold_evaluation(X, y, hyperparameters):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y[0])):\n",
    "        print(f\"\\nStratified K-Fold Iteration {i+1}/{skf.n_splits}\")\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train_challenge, y_test_challenge = y[0][train_index], y[0][test_index]  # Challenge\n",
    "        y_train_engagement, y_test_engagement = y[1][train_index], y[1][test_index]\n",
    "        \n",
    "        # Apply SMOTE only on the training data\n",
    "        print(\"Applying SMOTE to the training data...\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_train_resampled_challenge, y_train_resampled_challenge = smote.fit_resample(X_train_flat, y_train_challenge)\n",
    "        X_train_resampled_challenge = X_train_resampled_challenge.reshape(-1, X_train.shape[1], X_train.shape[2])  # 원래의 3D 형태로 복원\n",
    "        print(f\"After SMOTE - Challenge training data shape: {X_train_resampled_challenge.shape}, Challenge training labels shape: {y_train_resampled_challenge.shape}\")\n",
    "        \n",
    "        # Engagement 레이블에 대해 SMOTE 적용\n",
    "        print(\"Applying SMOTE to the training data (Engagement)...\")\n",
    "        X_train_resampled_engagement, y_train_resampled_engagement = smote.fit_resample(X_train_flat, y_train_engagement)\n",
    "        X_train_resampled_engagement = X_train_resampled_engagement.reshape(-1, X_train.shape[1], X_train.shape[2])  # 원래의 3D 형태로 복원\n",
    "        print(f\"After SMOTE - Engagement training data shape: {X_train_resampled_engagement.shape}, Engagement training labels shape: {y_train_resampled_engagement.shape}\")\n",
    "        \n",
    "        # Challenge와 Engagement 레이블의 길이를 맞추기 위해 두 레이블의 샘플 수를 동일하게 설정\n",
    "        min_length = min(len(y_train_resampled_challenge), len(y_train_resampled_engagement))\n",
    "        X_train_resampled_challenge = X_train_resampled_challenge[:min_length]\n",
    "        X_train_resampled_engagement = X_train_resampled_engagement[:min_length]\n",
    "        y_train_resampled_challenge = y_train_resampled_challenge[:min_length]\n",
    "        y_train_resampled_engagement = y_train_resampled_engagement[:min_length]\n",
    "        \n",
    "        # Encode labels (using the pre-fitted encoder)\n",
    "        y_train_cat_challenge = to_categorical(y_train_resampled_challenge)\n",
    "        y_test_cat_challenge = to_categorical(y_test_challenge)\n",
    "        \n",
    "        y_train_cat_engagement = to_categorical(y_train_resampled_engagement)\n",
    "        y_test_cat_engagement = to_categorical(y_test_engagement)\n",
    "        \n",
    "        # EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = create_multi_output_model_with_levels(\n",
    "            input_shape=(X_train_resampled_challenge.shape[1], X_train_resampled_challenge.shape[2]),\n",
    "            lstm_units=hyperparameters['lstm_units'],\n",
    "            dense_units=hyperparameters['dense_units'],\n",
    "            dropout_rate=hyperparameters['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            X_train_resampled_challenge, [y_train_cat_challenge, y_train_cat_engagement],\n",
    "            epochs=hyperparameters['epochs'],\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            validation_data=(X_test, [y_test_cat_challenge, y_test_cat_engagement]),\n",
    "            verbose=1  # Set to 1 for more detailed output\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['challenge_output_loss'], label='Challenge Output Loss (Training)')\n",
    "        plt.plot(history.history['val_challenge_output_loss'], label='Challenge Output Loss (Validation)')\n",
    "        plt.plot(history.history['engagement_output_loss'], label='Engagement Output Loss (Training)')\n",
    "        plt.plot(history.history['val_engagement_output_loss'], label='Engagement Output Loss (Validation)')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Fold {i+1} - Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['engagement_output_accuracy'], label='Training Engagement Accuracy')\n",
    "        plt.plot(history.history['val_engagement_output_accuracy'], label='Validation Engagement Accuracy')\n",
    "        plt.plot(history.history['challenge_output_accuracy'], label='Training Challenge Accuracy')\n",
    "        plt.plot(history.history['val_challenge_output_accuracy'], label='Validation Challenge Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Fold {i+1} - Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        print(\"Evaluating model on validation data...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_challenge_classes = np.argmax(y_pred[0], axis=1)\n",
    "        y_pred_engagement_classes = np.argmax(y_pred[1], axis=1)\n",
    "        y_test_challenge_classes = np.argmax(y_test_cat_challenge, axis=1)\n",
    "        y_test_engagement_classes = np.argmax(y_test_cat_engagement, axis=1)\n",
    "        \n",
    "        challenge_accuracy = accuracy_score(y_test_challenge_classes, y_pred_challenge_classes)\n",
    "        engagement_accuracy = accuracy_score(y_test_engagement_classes, y_pred_engagement_classes)\n",
    "        fold_accuracies.append((challenge_accuracy, engagement_accuracy))\n",
    "        print(f\"Challenge Accuracy for fold {i+1}: {challenge_accuracy:.4f}\")\n",
    "        print(f\"Engagement Accuracy for fold {i+1}: {engagement_accuracy:.4f}\")\n",
    "        \n",
    "        print(\"Classification Report for Challenge Output:\")\n",
    "        print(classification_report(y_test_challenge_classes, y_pred_challenge_classes))\n",
    "        \n",
    "        print(\"Classification Report for Engagement Output:\")\n",
    "        print(classification_report(y_test_engagement_classes, y_pred_engagement_classes))\n",
    "        \n",
    "        \n",
    "        cm_challenge = confusion_matrix(y_test_challenge_classes, y_pred_challenge_classes)\n",
    "        disp_challenge = ConfusionMatrixDisplay(confusion_matrix=cm_challenge)\n",
    "        disp_challenge.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f'Challenge Confusion Matrix for fold {i+1}')\n",
    "        plt.show()\n",
    "\n",
    "        # Confusion Matrix for Engagement\n",
    "        cm_engagement = confusion_matrix(y_test_engagement_classes, y_pred_engagement_classes)\n",
    "        disp_engagement = ConfusionMatrixDisplay(confusion_matrix=cm_engagement)\n",
    "        disp_engagement.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f'Engagement Confusion Matrix for fold {i+1}')\n",
    "        plt.show()\n",
    "    \n",
    "    return np.mean(fold_accuracies), np.std(fold_accuracies)\n",
    "\n",
    "def main():\n",
    "    directory = r'C:\\Users\\USER\\Downloads\\pilotdata'\n",
    "    data_frames, annotations = read_csv_files(directory)\n",
    "    \n",
    "    window_size = 10  # 10-second window\n",
    "    n_classes = 5  # Only 5-class classification\n",
    "    expected_features = 28  # Set this to the number of features you expect\n",
    "    baseline_duration = 30  # Set this to your baseline duration in seconds\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running 5-class classification with Loso evaluation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    X, challenge, engagement, participants = create_dataset(data_frames, annotations, window_size, expected_features, baseline_duration)\n",
    "    \n",
    "    if X is None or challenge is None or engagement is None or participants is None:\n",
    "        print(\"Failed to create dataset.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Number of sequences: {X.shape[0]}\")\n",
    "    print(f\"Sequence length: {X.shape[1]}\")\n",
    "    print(f\"Number of features: {X.shape[2]}\")\n",
    "    print(f\"Class distribution: {np.unique(challenge, return_counts=True)}\")\n",
    "    print(f\"Class distribution: {np.unique(engagement, return_counts=True)}\")\n",
    "    \n",
    "    # Encode labels once\n",
    "    le_challenge = LabelEncoder()\n",
    "    le_challenge.fit(challenge)  # Fit on the entire challenge dataset to ensure consistency\n",
    "    challenge_encoded = le_challenge.transform(challenge)\n",
    "    \n",
    "    le_engagement = LabelEncoder()\n",
    "    le_engagement.fit(engagement)  # Fit on the entire engagement dataset to ensure consistency\n",
    "    engagement_encoded = le_engagement.transform(engagement)\n",
    "    \n",
    "    print(\"\\nAfter Label Encoding:\")\n",
    "    print(f\"Challenge classes: {le_challenge.classes_}\")\n",
    "    print(f\"Encoded challenge labels: {np.unique(challenge_encoded, return_counts=True)}\")\n",
    "    print(f\"Engagement classes: {le_engagement.classes_}\")\n",
    "    print(f\"Encoded engagement labels: {np.unique(engagement_encoded, return_counts=True)}\")\n",
    "\n",
    "    \n",
    "    # Use the best hyperparameters you provided\n",
    "    best_hyperparameters = {\n",
    "        'lstm_units': (32, 16),\n",
    "        'dense_units': 64,\n",
    "        'dropout_rate': 0.3,\n",
    "        'epochs': 50,\n",
    "        'batch_size': 256\n",
    "    }\n",
    "    \n",
    "    print(f'challenge_encoded : {challenge_encoded.shape}')\n",
    "    print(f'engagement_encoded : {engagement_encoded.shape}')\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStarting Loso evaluation with best hyperparameters...\")\n",
    "        loso_mean, loso_std = loso_evaluation(X, [challenge_encoded, engagement_encoded], participants, best_hyperparameters)\n",
    "        print(f\"\\nLoso evaluation results:\")\n",
    "        print(f\"Mean accuracy: {loso_mean:.4f} (+/- {loso_std:.4f})\")\n",
    "        \n",
    "        # Optionally, train a final model on the entire dataset\n",
    "        # Apply SMOTE to the entire dataset (if desired), but be cautious of data leakage\n",
    "        # Here, we'll skip SMOTE for the final model to avoid potential leakage\n",
    "        final_model = create_multi_output_model_with_levels(\n",
    "            input_shape=(X.shape[1], X.shape[2]),\n",
    "            lstm_units=best_hyperparameters['lstm_units'],\n",
    "            dense_units=best_hyperparameters['dense_units'],\n",
    "            dropout_rate=best_hyperparameters['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        y_cat = [to_categorical(challenge_encoded, num_classes=n_classes), \n",
    "                 to_categorical(engagement_encoded, num_classes=n_classes)]\n",
    "        \n",
    "        print(\"\\nTraining final model on all data...\")\n",
    "        early_stopping_final = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "        final_model.fit(X, y_cat, epochs=best_hyperparameters['epochs'], \n",
    "                       batch_size=best_hyperparameters['batch_size'], \n",
    "                       callbacks=[early_stopping_final], verbose=1)\n",
    "        \n",
    "        # Save the final model\n",
    "        model_filename = r\"C:\\Users\\USER\\Downloads\\pilotdata\\best_lstm_model_5class1127.h5\"\n",
    "        final_model.save(model_filename)\n",
    "        print(f\"\\nFinal model saved as {model_filename}\")\n",
    "        \n",
    "        # Save the label encoder\n",
    "        import joblib\n",
    "        le_filename = r\"C:\\Users\\USER\\Downloads\\pilotdata\\label_encoder_5class1127.joblib\"\n",
    "        joblib.dump([le_challenge, le_engagement], le_filename)\n",
    "        print(f\"Label encoder saved as {le_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An error occurred during the Loso evaluation or model saving:\")\n",
    "        print(str(e))\n",
    "        print(\"Traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
